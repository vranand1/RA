{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 10K Puller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of the code uses CIKs as an input and constructs the URLs for the relevant 10K files (that meets three conditions: CIK + filed in 2016 + 10-K's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.1: Navigating the Edgar Search Results Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing search criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Code can be easily changed to read all the CIK's in a local .txt file. However, for the purpose of this assignment, I have deliberately avoided sending an additional .txt file (that would have all the CIK's). I have hard coded the CIK's in the code itself. In real world, it is much easier to save all the CIKs in a local .txt file and allow code to automatically read it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "cikList = [\"4962\",\"320193\",\"732712\",\"12927\",\"18230\",\"19617\",\"93410\",\"21344\",\"1001039\",\"30554\",\"34088\",\"40545\",\"354950\",\"50863\",\"51143\",\"200406\",\"63908\",\"310158\",\"66740\",\"320187\",\"78003\",\"80424\",\"731766\",\"101829\",\"104169\",\"1618921\",\"789019\",\"858877\",\"86312\",\"886982\",\"1403161\"]\n",
    "filingType = \"10-K\"\n",
    "dateBefore = \"20170101\"\n",
    "count = \"10\"\n",
    "yearOfInterest = \"2016\"\n",
    "urlBase = \"https://www.sec.gov/\"\n",
    "urlPt1 = \"cgi-bin/browse-edgar?action=getcompany&CIK=\"\n",
    "urlPt2 = \"&type=\"\n",
    "urlPt3 = \"&dateb=\"\n",
    "urlPt4 = \"&owner=exclude&count=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000004962', '0000320193'] \n",
      "\n",
      "['https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000004962&type=10-K&dateb=20170101&owner=exclude&count=10', 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K&dateb=20170101&owner=exclude&count=10']\n"
     ]
    }
   ],
   "source": [
    "# PADDING CIKS WITH ADDITIONAL 0S\n",
    "cikListPadded = [cik.zfill(10) for cik in cikList]\n",
    "print(cikListPadded[0:2], \"\\n\")\n",
    "\n",
    "# OBTAINING URL LIST\n",
    "urlList = [urlBase + urlPt1 + cik + urlPt2 + filingType + urlPt3 \n",
    "           + dateBefore + urlPt4 + count for cik in cikListPadded]\n",
    "print(urlList[0:2])\n",
    "\n",
    "# YEAR REGEX PATTERN\n",
    "yearRegex = yearOfInterest + \"-[0-9][0-9]-[0-9][0-9]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting search result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING WITH 1 URL AT A TIME\n",
    "testUrl = urlList[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(testUrl)\n",
    "pageSoup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableCandidates = pageSoup.find_all(\"table\", class_= \"tableFile2\")\n",
    "\n",
    "#ENSURING ONLY ONE TABLE IS READ\n",
    "if len(tableCandidates) == 1:\n",
    "    resultTable = tableCandidates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001403161&type=10-K&dateb=20170101&owner=exclude&count=10\n"
     ]
    }
   ],
   "source": [
    "print(testUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the required rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "requiredRows = []\n",
    "\n",
    "for tableRow in resultTable.find_all(\"tr\"):\n",
    "    for tableCell in tableRow.find_all(\"td\"):\n",
    "        if re.match(yearRegex, str(tableCell.contents[0])) is not None:\n",
    "            requiredRows.append(tableRow)\n",
    "            \n",
    "print(len(requiredRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the link to Filing Details page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.sec.gov//Archives/edgar/data/1403161/000140316116000058/0001403161-16-000058-index.htm']\n"
     ]
    }
   ],
   "source": [
    "filingLinks = []\n",
    "\n",
    "for row in requiredRows:\n",
    "    cell = row.find_all(\"a\", id = \"documentsbutton\")\n",
    "    link = urlBase + cell[0][\"href\"]\n",
    "    filingLinks.append(link)\n",
    "    \n",
    "print(filingLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.2: Navigating Filing Links Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links10K = []\n",
    "\n",
    "for link in filingLinks:\n",
    "    filingPage = requests.get(link)\n",
    "    filingPageSoup = BeautifulSoup(filingPage.content, \"html.parser\")\n",
    "    \n",
    "    # EXTRACTING TABLE\n",
    "    docTable = filingPageSoup.find_all(\"table\", summary = \"Document Format Files\")[0]\n",
    "\n",
    "    # EXTRACING REQUIRED ROW\n",
    "    row10K = None\n",
    "    for row in docTable.find_all(\"tr\"):\n",
    "        for tableCell in row.find_all(\"td\"):\n",
    "            if re.search(\"10-K\", str(tableCell.contents)):\n",
    "                row10K = row\n",
    "                break\n",
    "    \n",
    "    # EXTRACTING REQUIRED LINK\n",
    "    link = row10K.find_all(\"a\")[0][\"href\"]\n",
    "    link = urlBase + link\n",
    "    links10K.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.sec.gov//Archives/edgar/data/1403161/000140316116000058/v093016.htm']\n"
     ]
    }
   ],
   "source": [
    "print(links10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.3: Testing link validity by writing the file to a test output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPage = requests.get(links10K[0])\n",
    "testPageSoup = BeautifulSoup(testPage.content, \"html.parser\")\n",
    "\n",
    "with open(\"testop.html\", \"w\", encoding = \"utf-8\") as file:\n",
    "    file.write(str(testPageSoup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
